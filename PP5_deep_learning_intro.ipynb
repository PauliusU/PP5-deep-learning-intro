{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87b40cd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PP5: deep learning intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ffd26",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PauliusU/PP5-deep-learning-intro/blob/master/PP5_deep_learning_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad255c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Choosing and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3d5d9",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Project requires to use use dataset which suitable for non-linearly separable classification.\n",
    "\n",
    "The famous [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) contains three classes, Setosa is linearly separable from the other two classes. Versicolor and Virginica classes are not linearly separable (see [source](https://www.researchgate.net/figure/Iris-data-set-There-are-three-classes-Setosa-class-is-linearly-separable-from-the-other_fig2_300643220)).\n",
    "\n",
    "Therefore, the Iris dataset is suitable for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce055553",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DESCR': '.. _iris_dataset:\\n'\n",
      "          '\\n'\n",
      "          'Iris plants dataset\\n'\n",
      "          '--------------------\\n'\n",
      "          '\\n'\n",
      "          '**Data Set Characteristics:**\\n'\n",
      "          '\\n'\n",
      "          '    :Number of Instances: 150 (50 in each of three classes)\\n'\n",
      "          '    :Number of Attributes: 4 numeric, predictive attributes and the '\n",
      "          'class\\n'\n",
      "          '    :Attribute Information:\\n'\n",
      "          '        - sepal length in cm\\n'\n",
      "          '        - sepal width in cm\\n'\n",
      "          '        - petal length in cm\\n'\n",
      "          '        - petal width in cm\\n'\n",
      "          '        - class:\\n'\n",
      "          '                - Iris-Setosa\\n'\n",
      "          '                - Iris-Versicolour\\n'\n",
      "          '                - Iris-Virginica\\n'\n",
      "          '                \\n'\n",
      "          '    :Summary Statistics:\\n'\n",
      "          '\\n'\n",
      "          '    ============== ==== ==== ======= ===== ====================\\n'\n",
      "          '                    Min  Max   Mean    SD   Class Correlation\\n'\n",
      "          '    ============== ==== ==== ======= ===== ====================\\n'\n",
      "          '    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n'\n",
      "          '    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n'\n",
      "          '    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n'\n",
      "          '    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n'\n",
      "          '    ============== ==== ==== ======= ===== ====================\\n'\n",
      "          '\\n'\n",
      "          '    :Missing Attribute Values: None\\n'\n",
      "          '    :Class Distribution: 33.3% for each of 3 classes.\\n'\n",
      "          '    :Creator: R.A. Fisher\\n'\n",
      "          '    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n'\n",
      "          '    :Date: July, 1988\\n'\n",
      "          '\\n'\n",
      "          'The famous Iris database, first used by Sir R.A. Fisher. The '\n",
      "          'dataset is taken\\n'\n",
      "          \"from Fisher's paper. Note that it's the same as in R, but not as in \"\n",
      "          'the UCI\\n'\n",
      "          'Machine Learning Repository, which has two wrong data points.\\n'\n",
      "          '\\n'\n",
      "          'This is perhaps the best known database to be found in the\\n'\n",
      "          \"pattern recognition literature.  Fisher's paper is a classic in the \"\n",
      "          'field and\\n'\n",
      "          'is referenced frequently to this day.  (See Duda & Hart, for '\n",
      "          'example.)  The\\n'\n",
      "          'data set contains 3 classes of 50 instances each, where each class '\n",
      "          'refers to a\\n'\n",
      "          'type of iris plant.  One class is linearly separable from the other '\n",
      "          '2; the\\n'\n",
      "          'latter are NOT linearly separable from each other.\\n'\n",
      "          '\\n'\n",
      "          '.. topic:: References\\n'\n",
      "          '\\n'\n",
      "          '   - Fisher, R.A. \"The use of multiple measurements in taxonomic '\n",
      "          'problems\"\\n'\n",
      "          '     Annual Eugenics, 7, Part II, 179-188 (1936); also in '\n",
      "          '\"Contributions to\\n'\n",
      "          '     Mathematical Statistics\" (John Wiley, NY, 1950).\\n'\n",
      "          '   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and '\n",
      "          'Scene Analysis.\\n'\n",
      "          '     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page '\n",
      "          '218.\\n'\n",
      "          '   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New '\n",
      "          'System\\n'\n",
      "          '     Structure and Classification Rule for Recognition in Partially '\n",
      "          'Exposed\\n'\n",
      "          '     Environments\".  IEEE Transactions on Pattern Analysis and '\n",
      "          'Machine\\n'\n",
      "          '     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n'\n",
      "          '   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE '\n",
      "          'Transactions\\n'\n",
      "          '     on Information Theory, May 1972, 431-433.\\n'\n",
      "          '   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s '\n",
      "          'AUTOCLASS II\\n'\n",
      "          '     conceptual clustering system finds 3 classes in the data.\\n'\n",
      "          '   - Many, many more ...',\n",
      " 'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]),\n",
      " 'data_module': 'sklearn.datasets.data',\n",
      " 'feature_names': ['sepal length (cm)',\n",
      "                   'sepal width (cm)',\n",
      "                   'petal length (cm)',\n",
      "                   'petal width (cm)'],\n",
      " 'filename': 'iris.csv',\n",
      " 'frame': None,\n",
      " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
      " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10')}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Initiate Iris flower dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Get to know the structure of the dataset\n",
    "pprint(iris)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e40ae0e",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/pauliusu/Dropbox/code/AI-PP5-deep-learning-intro/PP5_deep_learning_intro.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pauliusu/Dropbox/code/AI-PP5-deep-learning-intro/PP5_deep_learning_intro.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m OneHotEncoder, StandardScaler\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pauliusu/Dropbox/code/AI-PP5-deep-learning-intro/PP5_deep_learning_intro.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Split the dataset into X and y\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pauliusu/Dropbox/code/AI-PP5-deep-learning-intro/PP5_deep_learning_intro.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X \u001b[39m=\u001b[39m iris[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pauliusu/Dropbox/code/AI-PP5-deep-learning-intro/PP5_deep_learning_intro.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m y \u001b[39m=\u001b[39m iris[\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pauliusu/Dropbox/code/AI-PP5-deep-learning-intro/PP5_deep_learning_intro.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m names \u001b[39m=\u001b[39m iris[\u001b[39m'\u001b[39m\u001b[39mtarget_names\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iris' is not defined"
     ]
    }
   ],
   "source": [
    "# Data preparation and splitting the dataset\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Split the dataset into X and y\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "# Verify X and y\n",
    "print(X[:5])\n",
    "print(y[:5])\n",
    "\n",
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "Y = enc.fit_transform(y[:, np.newaxis]).toarray()\n",
    "\n",
    "# Use a StandardScaler to remove the mean and scale the features to unit variance.\n",
    "# In other words, scale data to have mean 0 and variance 1  which is important\n",
    "# for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n",
    "print(len(X_scaled))\n",
    "\n",
    "# Split the data set into training and testing sets. Use 80% and 20% split.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y, test_size=0.5, random_state=2)\n",
    "\n",
    "# Verify splitting results\n",
    "print(X_train.shape)  # (120, 4)\n",
    "print(X_test.shape)  # (30, 4)\n",
    "print(Y_train.shape)  # (120, 3)\n",
    "print(Y_test.shape)  # (30, 3)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46930a",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aca7f9",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    plt.plot(X_plot[:, 0], X_plot[:, 1], linestyle='none', marker='o', label=target_name)\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.axis('equal')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    plt.plot(X_plot[:, 2], X_plot[:, 3], linestyle='none', marker='o', label=target_name)\n",
    "plt.xlabel(feature_names[2])\n",
    "plt.ylabel(feature_names[3])\n",
    "plt.axis('equal')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8300a4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba789867",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "def create_custom_model(input_dim, output_dim, nodes, n=1, name='model'):\n",
    "    def create_model():\n",
    "        # Create model\n",
    "        model = Sequential(name=name)\n",
    "        for i in range(n):\n",
    "            model.add(Dense(nodes, input_dim=input_dim, activation='relu'))\n",
    "        model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    return create_model\n",
    "\n",
    "\n",
    "models = [create_custom_model(n_features, n_classes, 8, i, f'model_{i}')\n",
    "          for i in range(1, 4)]\n",
    "\n",
    "for create_model in models:\n",
    "    create_model().summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101226b7",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training of the models with Keras\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "history_dict = {}\n",
    "\n",
    "# Use Tensorboard as a callback if we want to explore the model and the outputs in detail\n",
    "cb = TensorBoard()\n",
    "\n",
    "for create_model in models:\n",
    "    model = create_model()\n",
    "    print('Model name:', model.name)\n",
    "    history_callback = model.fit(X_train, Y_train,\n",
    "                                 batch_size=5,\n",
    "                                 epochs=50,\n",
    "                                 verbose=0,\n",
    "                                 validation_data=(X_test, Y_test),\n",
    "                                 callbacks=[cb])\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(f'Test loss: {score[0]}')\n",
    "    print(f'Test accuracy: {score[1]}')\n",
    "    print('*' * 80)\n",
    "\n",
    "    history_dict[model.name] = [history_callback, model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78180e6f",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize performance of the models created by Keras\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(8, 6))\n",
    "\n",
    "for model_name in history_dict:\n",
    "    val_accurady = history_dict[model_name][0].history['val_accuracy']\n",
    "    val_loss = history_dict[model_name][0].history['val_loss']\n",
    "    ax1.plot(val_accurady, label=model_name)\n",
    "    ax2.plot(val_loss, label=model_name)\n",
    "\n",
    "ax1.set_ylabel('validation accuracy')\n",
    "ax2.set_ylabel('validation loss')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax1.legend()\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69198a76",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Show Receiver Operating Characteristic (ROC) curve how well the models perform. \n",
    "The ROC plot compares the false positive rate with the true positive rate. \n",
    "\n",
    "Also compute for each model the Area under the curve (AUC). \n",
    "auc = 1  means perfect classification while auc = 0.5 is random guessing.\n",
    "\"\"\"\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "for model_name in history_dict:\n",
    "    model = history_dict[model_name][1]\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    fpr, tpr, threshold = roc_curve(y_test.ravel(), Y_pred.ravel())\n",
    "\n",
    "    plt.plot(fpr, tpr, label='{}, AUC = {:.3f}'.format(\n",
    "        model_name, auc(fpr, tpr)))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9fa4f",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_3 usually showed the best results. \n",
    "So let's measure performance with 10-fold cross validation for the model_3 by using the KerasClassifier.\n",
    "KerasClassifier is a useful Wrapper when using Keras together with scikit-learn.\n",
    "\"\"\"\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "create_model = create_custom_model(n_features, n_classes, 8, 3)\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_model,\n",
    "                            epochs=100, batch_size=5, verbose=0)\n",
    "scores = cross_val_score(estimator, X_scaled, Y, cv=10)\n",
    "print(f\"Accuracy : {scores.mean():0.2f} (+/- {scores.std():0.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859906ac",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfdc201",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a628cc",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setup model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 50)\n",
    "        self.layer2 = nn.Linear(50, 50)\n",
    "        self.layer3 = nn.Linear(50, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf9382",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(X_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaed0e0",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model training\n",
    "# tqdm is used here to track progress more efficiently\n",
    "\n",
    "import tqdm\n",
    "\n",
    "EPOCHS  = 300\n",
    "X_train = Variable(torch.from_numpy(X_train)).float()\n",
    "y_train = Variable(torch.from_numpy(y_train)).long()\n",
    "X_test  = Variable(torch.from_numpy(X_test)).float()\n",
    "y_test  = Variable(torch.from_numpy(y_test)).long()\n",
    "\n",
    "loss_list     = np.zeros((EPOCHS,))\n",
    "accuracy_list = np.zeros((EPOCHS,))\n",
    "\n",
    "for epoch in tqdm.trange(EPOCHS):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss_list[epoch] = loss.item()\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
    "        accuracy_list[epoch] = correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749eebd2",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Accuracy and Loss from training\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "ax1.plot(accuracy_list)\n",
    "ax1.set_ylabel(\"validation accuracy\")\n",
    "ax2.plot(loss_list)\n",
    "ax2.set_ylabel(\"validation loss\")\n",
    "ax2.set_xlabel(\"epochs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24aa996",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Repeat plotting ROC an AUC as it was done with Keras\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# One hot encoding\n",
    "# To prepare the test data, we need to use the OneHotEncoder to encode the integer \n",
    "# features into a One-hot vector which we then flatten with numpy.ravel() for sklearn.metrics.roc_curve().\n",
    "enc = OneHotEncoder()\n",
    "Y_onehot = enc.fit_transform(y_test[:, np.newaxis]).toarray()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).numpy()\n",
    "    fpr, tpr, threshold = roc_curve(Y_onehot.ravel(), y_pred.ravel())\n",
    "    \n",
    "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc(fpr, tpr)))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85490a36",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Which framework is preferable?\n",
    "\n",
    "When choose between `Keras` and `PyTorch` I would prefer `Keras` framework work. This is based mostly on subjective experience when facing issues with both of them. I was able to find solutions and fixes for `Keras` more easily. `PyTorch` took me longer to get started and to deal with Python errors.\n",
    "\n",
    "In addition to that, syntax of `Keras` seems to be more readable and concise. Consequently, compared to `PyTorch`, `Keras` appeared to be more intuitive and beginner friendly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('AI-PP5-deep-learning-intro-WpvXuOuE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9ca47e1710a9bfdfa93ef71640a60bc6fa73c41d036d05953395e286c06b091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
